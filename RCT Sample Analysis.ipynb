{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation and Analysis\n",
    "\n",
    "This notebook runs a simulation configuration that was used in our analysis with the Tijuana Red Cross. This was prepared for public viewing. A simulation is run using the ambulance selection policy of best travel time, and uses the scenario of Weekend with Disaster. The historical data used for this demonstration is generated on the spot in order to avoid using the RCT's data. For more information on how the `ems-simulator` works, see https://github.com/EMSTrack/EMS-Simulator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation of RCT EMS v3\n",
    "\n",
    "Three scenarios: \n",
    "\n",
    "1. weekday\n",
    "2. weekend\n",
    "3. weekend with disaster\n",
    "\n",
    "Three dispatch policies for ambulance selection: \n",
    "1. fastest ambulance \n",
    "2. least coverage disruption\n",
    "3. optimal by weighting travel time and coverage\n",
    "\n",
    "Nine total simulations, each compared. \n",
    "\n",
    "This notebook is a version of the notebook made for demonstrations on binder. \n",
    "## TODO CONVERT THIS INTO A SAMPLE DISASTER SCENARIO RUN AND THEN USE ITS DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case count distributions\n",
    "\n",
    "Each simulation has 100 cases regardless of how the scenarios will play out. This should mean that the disaster scenario will finish 100 cases in less time because there is a disaster time slot where many cases occur. We want to see if this is the case by comparing the disaster scenarios against the weekday and weekend scenarios. The dispatch policy might affect the distributions, but probably not be too much. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cases reached before r1: 10 minutes\n",
    "\n",
    "Since previous RCT studies looked at using the US EMS Act as the standard, we want to know how many cases are reached before the 10 minute requirement. If this is not reached, it's still helpful to know if the case was reached before the 14 minute secondary coverage requirement. \n",
    "\n",
    "Since Pons stated 8 minutes is the time when statistics actually found significant difference in case effectiveness, it may be useful to count the cases reached before 8 minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double coverage \n",
    "\n",
    "Plot the double coverage of the simulated RCTs over time. Here, the dispatch policy is important, and the specific scenarios may or may not make a difference. We expect to see best travel to do the worst, but weighted should have similar shape with best travel time despite being a little better. \n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ems.simulators.event_simulator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4911332681af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDriver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDriver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'examples/tj-disaster-best-travel.yaml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0msim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_simulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/ems/run.py\u001b[0m in \u001b[0;36mcreate_simulator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_simulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'simulator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/ems/run.py\u001b[0m in \u001b[0;36m_create_objects\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mobjects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mobjects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_recurse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/ems/run.py\u001b[0m in \u001b[0;36m_create_recurse\u001b[0;34m(param, objects)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;31m# print(\"Instantiating: {}\".format(cname)) # TODO Change to log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ems.simulators.event_simulator'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import scipy.stats as stats\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "import numpy as np\n",
    "from numpy import median, mean\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from functools import reduce\n",
    "from collections import Counter\n",
    "import yaml \n",
    "\n",
    "from ems.run import Driver\n",
    "driver = Driver('examples/tj-disaster-best-travel.yaml')\n",
    "sim, data = driver.create_simulator()\n",
    "results = sim.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#       READ DATA INTO THE NOTEBOOK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# csvs = [ # Every simulation produces these files.\n",
    "#     \"chosen_ambulances.csv\",\n",
    "#     \"chosen_bases.csv\",\n",
    "#     \"chosen_hospitals.csv\",\n",
    "#     \"metrics.csv\",\n",
    "#     \"processed_cases.csv\",\n",
    "# ]\n",
    "\n",
    "\n",
    "# scenarios = [ # Three scenarios, two from the RCT, one additional contributed scenario \n",
    "#     \"weekday\",\n",
    "#     \"weekend\",\n",
    "#     \"disaster\", # Note that the base scenario is weekend with a disaster intejected inside\n",
    "# ]\n",
    "\n",
    "# policies = [ # Three ambulance dispatch policies were tested \n",
    "#     \"best-travel\",\n",
    "#     \"best-coverage\",\n",
    "#     \"weighted-dispatch\",\n",
    "# ]\n",
    "\n",
    "# result_directories = [ # This notebook is assumed to be in the home project repository directory. \n",
    "#     \"tj-{scenario}-{policy}/\".format( scenario=scenario, policy=policy) \\\n",
    "#     for scenario in scenarios for policy in policies\n",
    "# ]\n",
    "\n",
    "# result_directories += [\n",
    "#     '/scaling-weeks-best-travel/', \n",
    "#     '/scaling-weeks-best-coverage/', \n",
    "#     '/depleting-until-zero/',\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "# data_files = [\n",
    "#     directory + csv \\\n",
    "#     for directory in result_directories for csv in csvs\n",
    "# ]\n",
    "\n",
    "# def read_folder(data_files):\n",
    "    \n",
    "#     \"\"\"\n",
    "#     For each folder (simulation), read in their csv results\n",
    "    \n",
    "#     data_files: a list of strs pointing to each csv file in the results\n",
    "#     returns: a dict(k: scenario, policy, data_name, v: pandas representation of the csv)\n",
    "#     \"\"\"\n",
    "    \n",
    "#     assert isinstance(data_files, list) \n",
    "#     for data in data_files: assert isinstance(data, str)\n",
    "#     return {data: pandas.read_csv(\"../results/\" + data) for data in data_files}\n",
    "\n",
    "\n",
    "# dataset = read_folder(data_files) # Now the source of truth for data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMERGENCY CASE DISTRIBUTIONS DRAWN OVER THE ZONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "title = \"Location of Generated Emergencies\"\n",
    "\n",
    "# Read the polygons and draw the matplotlibs. Return lat longs, as longs lats (xs ys)\n",
    "xs = yaml.full_load(open('../../longitudes.yaml'))\n",
    "ys = yaml.full_load(open('../../latitudes.yaml'))\n",
    "\n",
    "for i in range(len(xs)):\n",
    "    plt.plot(xs[i] + [xs[i][0]], ys[i] + [ys[i][0]],  '-', color='black')\n",
    "\n",
    "\n",
    "# processed cases for each simulation \n",
    "cases = {k:v for k,v in dataset.items() if \"processed_cases\" in k}\n",
    "\n",
    "gen_cases = {filename: cases[filename] for filename in cases \\\n",
    "             if \"best-travel\" in filename and \"tj\" in filename}\n",
    "\n",
    "colors = ['green', 'blue', 'red']\n",
    "\n",
    "num_cases = 200\n",
    "\n",
    "for case_name in gen_cases:\n",
    "    label = case_name.split('/')[0].replace('-', ' ')\n",
    "    plt.plot(cases[case_name].longitude[0:num_cases], cases[case_name].latitude[0:num_cases], '.', \n",
    "             color=colors.pop(), label=label)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Take out the slicing from line 26 if want *every* case plotted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed cases for each simulation \n",
    "cases = {k:v for k,v in dataset.items() if \"processed_cases\" in k}\n",
    "\n",
    "gen_cases = {filename: cases[filename] for filename in cases if \"best-travel\" in filename}\n",
    "\n",
    "for name in gen_cases:\n",
    "    continue # UNCOMMENT TO DISPLAY SEPARATE GRAPHS\n",
    "    plt.figure(name)\n",
    "    plt.title(name)\n",
    "    for i in range(len(xs)):\n",
    "        plt.plot(xs[i] + [xs[i][0]], ys[i] + [ys[i][0]], )\n",
    "    plt.plot(gen_cases[name].longitude, gen_cases[name].latitude, '.', color='black')\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE COUNT DISTRIBUTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the functions necessary to accumulate each set of case datetimes \n",
    "# in order to bin them into histographic distributions.\n",
    "\n",
    "# Draw the histogram for each scenario, before drawing a histogram for all scenarios\n",
    "\n",
    "# processed cases for each simulation \n",
    "cases = {k:v for k,v in dataset.items() if \"processed_cases\" in k}\n",
    "\n",
    "def plot_histogram_basic(processed_cases, name):\n",
    "    \"\"\"\n",
    "    For each list of processed cases, draw the histogram distribution of cases per hour.\n",
    "    \"\"\"\n",
    "    assert isinstance(processed_cases, pandas.core.frame.DataFrame)\n",
    "    assert isinstance(name, str)\n",
    "    \n",
    "    # Logic towards the histogram \n",
    "    dates = [datetime.strptime(arr[1], '%Y-%m-%d %H:%M:%S.%f').replace(\n",
    "        minute=0, second=0, microsecond=0) for arr in processed_cases.values]\n",
    "    \n",
    "    first = dates[0]\n",
    "    deltas = [(date-first).total_seconds()/3600 for date in dates]\n",
    "    \n",
    "    c = Counter(deltas)\n",
    "    sum(c.values())\n",
    "\n",
    "    l = list(deltas)\n",
    "\n",
    "    # Logic towards the smooth histogram\n",
    "    smooth_xs = np.linspace(int(min(c.keys())), int(max(c.keys())), len(c.keys()))\n",
    "    spl = make_interp_spline(list(c.keys()), list(c.values()), k=3)\n",
    "    smooth_ys = spl(smooth_xs)\n",
    "    \n",
    "    \n",
    "    # Draw the blocky histogram\n",
    "    figure_title = \"Case count per hour: \" + name + \"\"\n",
    "    plt.figure(figure_title)\n",
    "    plt.title(figure_title)\n",
    "    ys, xs, patches = plt.hist(deltas, bins=int((max(deltas)+1)/1), histtype='step')\n",
    "    plt.xlabel(\"Number of hours since starting\"); plt.ylabel('Number of cases')\n",
    "\n",
    "    # Draw the smoothed histogram\n",
    "    plt.hist(deltas, bins=int((max(deltas)+1)), histtype='step', color='black')\n",
    "    plt.xlabel(\"Number of hours since starting\"); plt.ylabel('Number of cases')\n",
    "    # plt.ylim(0,10)\n",
    "    plt.plot(smooth_xs, smooth_ys, '-', color='red')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Draw each processed cases' case count. COMMENT OUT WHEN NOT NEEDED.\n",
    "for name, processed_cases in cases.items():\n",
    "#     plot_histogram_basic(processed_cases, name) \n",
    "    pass\n",
    "    \n",
    "    \n",
    "def histogram_collapsed(processed_cases, name):\n",
    "    \"\"\"\n",
    "    For each list of processed cases, draw the histogram distribution of cases per hour.\n",
    "    \"\"\"\n",
    "    assert isinstance(processed_cases, pandas.core.frame.DataFrame)\n",
    "    assert isinstance(name, str)\n",
    "    \n",
    "    # Logic towards the histogram \n",
    "    dates = [datetime.strptime(arr[1], '%Y-%m-%d %H:%M:%S.%f').replace(\n",
    "        minute=0, second=0, microsecond=0) for arr in processed_cases.values]\n",
    "    \n",
    "    first = dates[0]\n",
    "    deltas = [(date-first).total_seconds()/3600 for date in dates]\n",
    "    \n",
    "    c = Counter(deltas)\n",
    "    sum(c.values())\n",
    "\n",
    "    l = list(deltas)\n",
    "\n",
    "    # Logic towards the smooth histogram\n",
    "    smooth_xs = np.linspace(int(min(c.keys())), int(max(c.keys())), len(c.keys()))\n",
    "    spl = make_interp_spline(list(c.keys()), list(c.values()), k=3)\n",
    "    smooth_ys = spl(smooth_xs)\n",
    "    \n",
    "    \n",
    "    # Draw the blocky histogram\n",
    "    figure_title = \"Cases per hour, all\"\n",
    "    plt.xlabel(\"Number of hours since starting\"); plt.ylabel('Number of cases')\n",
    "    plt.xlim(0,24)\n",
    "\n",
    "    # Draw the smoothed histogram\n",
    "    plt.hist(deltas, bins=int((max(deltas)+1)), histtype='step')\n",
    "    plt.xlabel(\"Number of hours since starting\"); plt.ylabel('Number of cases')\n",
    "    plt.ylim(0,38)\n",
    "    label = name.split('/')[0].replace('-', ' ')\n",
    "\n",
    "    plt.plot(smooth_xs, smooth_ys, '-', label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_coverages='Coverages overlayed'\n",
    "plt.figure(all_coverages)\n",
    "plt.title(all_coverages)\n",
    "for name, processed_cases in cases.items():\n",
    "    if \"best-travel\" in name:\n",
    "        histogram_collapsed(processed_cases, name) \n",
    "        \n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "caption = \"So this one is interesting but we're showing that different scenarios results in different \\\n",
    "lengths of time required to finish solving that many cases. \"\n",
    "\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "1. Seems like the case distribution for each hour are the same. \n",
    "2. This is pretty expected, since the density of cases is not extremely high, we don't hit the case delay scenario too often (really, only in the disaster scenario). \n",
    "3. Because of that, the major differences here are between each scenario, which makes sense. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A plot for each scenario for total case duration or average case duration \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = {k:v for k,v in dataset.items() if \"processed_cases\" in k}\n",
    "\n",
    "# Grab the indices which store the durations\n",
    "indices = []\n",
    "for name, caseset in cases.items(): \n",
    "    keys = caseset.keys()\n",
    "    for index in range(len(keys)):\n",
    "        if 'duration' in keys[index]: \n",
    "            print(index, keys[index])\n",
    "            indices += [index]\n",
    "        \n",
    "    break\n",
    "\n",
    "# For each case, determine the total duration. List of casesets, which contain list of cases. Use transformations. \n",
    "\n",
    "cleaned_cases = {name: [{column_name: float(mean([pandas.to_timedelta(d).total_seconds()/60 for d in data]))} for column_name, data in caseset.items() if \"duration\" in column_name] for name, caseset in cases.items()}\n",
    "\n",
    "\n",
    "with open('mean_times.yaml', 'w') as output:\n",
    "    output.write(yaml.dump(cleaned_cases))\n",
    "\n",
    "total_average_durations = {name: sum([v for times_d in times for k, v in times_d.items()]) for name, times in cleaned_cases.items()}\n",
    "    \n",
    "with open('mean_times.yaml', 'a') as output:\n",
    "    output.write(yaml.dump(total_average_durations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_average_durations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case durations \n",
    "\n",
    "- There is a lot of variance due to the random generation of the at_location durations\n",
    "- In the disaster scenario, the average case duration with best travel time is almost just as bad as other two dispatch policies on normal days. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A plot for average case duration totals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"total average case durations\"\n",
    "plt.figure(name); plt.title(name)\n",
    "\n",
    "plt.xlabel(\"Scenario and dispatch policy\"); plt.ylabel(\"Minutes\")\n",
    "\n",
    "for name, duration in total_average_durations.items():\n",
    "    plt.bar(name, duration)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# A plot for average E1 durations only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1s = {}\n",
    "\n",
    "for name, durations in cleaned_cases.items():\n",
    "    \n",
    "    # Name only the scenario and the dispatch policy \n",
    "    name = name.split('/')[0].split('-')[1:4]\n",
    "    name = \" \".join(name)\n",
    "    \n",
    "    for d in durations:\n",
    "        for k, v in d.items():\n",
    "            if 'TO_INCIDENT' in k:\n",
    "                e1s[name] = v\n",
    "    \n",
    "e1s\n",
    "\n",
    "name = \"TO_INCIDENT durations\"\n",
    "\n",
    "plt.figure(name)\n",
    "plt.title(name)\n",
    "\n",
    "for k, v in e1s.items():\n",
    "    plt.bar(k, v, label=k)\n",
    "\n",
    "\n",
    "# plt.ylim(0,80)\n",
    "    \n",
    "plt.xlabel('Scenario and Policy')\n",
    "plt.ylabel(\"Minutes\")\n",
    "# plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REACHING EMERGENCY POINTS WITHIN TIME LIMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# to incident durations\n",
    "\n",
    "def plot_travel_times_basic(processed_cases, name):\n",
    "    \n",
    "    assert isinstance(processed_cases, pandas.core.frame.DataFrame)\n",
    "    assert isinstance(name, str)\n",
    "    \n",
    "    # Logic for counting cases reached within some time. \n",
    "    durations = [pandas.to_timedelta(array[7]) for array in processed_cases.values]\n",
    "    r1 = timedelta(minutes=10)\n",
    "    r2 = timedelta(minutes=14)\n",
    "    \n",
    "    times = {\n",
    "    'lt_r1' : list(filter(lambda t: t <  r1, durations)),\n",
    "    'lt_r2' : list(filter(lambda t: t <  r2 and t >= r1, durations)),\n",
    "    'beyond': list(filter(lambda t: t >= r2, durations)),\n",
    "    }\n",
    "\n",
    "    counts = {k:len(v) for k,v in times.items()}\n",
    "    \n",
    "    # Draw the bar graphs\n",
    "\n",
    "    figure_title = \"Travel times for the TO_INCIDENT events:\" + name\n",
    "\n",
    "    print(figure_title)\n",
    "    print(counts)\n",
    "\n",
    "# Draw each processed cases' travel times. COMMENT OUT WHEN NOT NEEDED.\n",
    "for name, processed_cases in cases.items():\n",
    "    plot_travel_times_basic(processed_cases, name) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments\n",
    "1. This one is actually really interesting. \n",
    "2. Least disruption is terrible in all scenarios and this makes sense because this dispatch policy goes against the whole point of EMS which is to **quickly** reach the patient.\n",
    "3. In the Weekdays scenario, there was no change between BTT and weighted.\n",
    "4. In the Weekends scenario, BTT did the best. However, we do not know whether it was **necessary** for this to happen. \n",
    "5. In the disaster scenario, weighted did the best. It seems like a few of the r2's became r1, and some other r2's went beyond r2. \n",
    "6. None of this paints a complete picture, because we're not only interested in how many cases were filled in r1, but we're also interested in, of these cases, how many **top priority** cases were reached with r1. \n",
    "\n",
    "Note: different scenarios produced a different set of cases so at the moment it's mainly just worth comparing within scenarios "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event.TO_INCIDENT for Priority 1 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Implement the exact same code as above, but filter for the Priority 1 cases. \n",
    "\n",
    "def plot_travel_times_priority1(processed_cases, name):\n",
    "    \n",
    "    assert isinstance(processed_cases, pandas.core.frame.DataFrame)\n",
    "    assert isinstance(name, str)\n",
    "    \n",
    "    # Logic for counting cases reached within some time. \n",
    "    durations = [pandas.to_timedelta(array[7]) for array in processed_cases.values \n",
    "                 if array[4] == 1]\n",
    "    r1 = timedelta(minutes=10)\n",
    "    r2 = timedelta(minutes=14)\n",
    "    \n",
    "    times = {\n",
    "    'lt_r1' : list(filter(lambda t: t <  r1, durations)),\n",
    "    'lt_r2' : list(filter(lambda t: t <  r2 and t >= r1, durations)),\n",
    "    'beyond': list(filter(lambda t: t >= r2, durations)),\n",
    "    }\n",
    "\n",
    "    counts = {k:len(v) for k,v in times.items()}\n",
    "    \n",
    "    # Draw the bar graphs\n",
    "\n",
    "    figure_title = \"Travel times for the TO_INCIDENT events:\" + name\n",
    "    plt.show()\n",
    "    print(figure_title)\n",
    "    print(counts)\n",
    "\n",
    "# Draw each processed cases' travel times. COMMENT OUT WHEN NOT NEEDED.\n",
    "for name, processed_cases in cases.items():\n",
    "    if \"best-coverage\" in name: \n",
    "        continue\n",
    "    plot_travel_times_priority1(processed_cases, name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AVERAGE TRAVEL TIME FOR EACH PRIORITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def average_travel_times_priority(processed_cases, name, priority):\n",
    "    \n",
    "    assert isinstance(processed_cases, pandas.core.frame.DataFrame)\n",
    "    assert isinstance(name, str)\n",
    "    \n",
    "    # Logic for counting cases reached within some time. \n",
    "    durations = [pandas.to_timedelta(array[7]) for array in processed_cases.values \n",
    "                 if array[4] == priority]\n",
    "    r1 = timedelta(minutes=10)\n",
    "    r2 = timedelta(minutes=14)\n",
    "    \n",
    "    times = {\n",
    "    'lt_r1' : list(filter(lambda t: t <  r1, durations)),\n",
    "    'lt_r2' : list(filter(lambda t: t <  r2 and t >= r1, durations)),\n",
    "    'beyond': list(filter(lambda t: t >= r2, durations)),\n",
    "    }\n",
    "\n",
    "    counts = {k:len(v) for k,v in times.items()}\n",
    "    avg = mean(durations).total_seconds()\n",
    "    total = sum([t.total_seconds() for t in durations])\n",
    "    \n",
    "    print(name, priority)\n",
    "    print(counts)\n",
    "    print(\"Average to_incident duration: \", avg)\n",
    "    print(\"Total time in seconds for E1: \", total)\n",
    "\n",
    "\n",
    "# Draw each processed cases' travel times. COMMENT OUT WHEN NOT NEEDED.\n",
    "for priority in range(1, 5):\n",
    "    for name, processed_cases in cases.items():\n",
    "        if \"tj\" not in name:\n",
    "            continue\n",
    "        if \"coverage\" in name: \n",
    "            continue\n",
    "        average_travel_times_priority(processed_cases, name, priority) \n",
    "        print()\n",
    "    print(\"#\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, after using the new formula for weighing the two dispatches, a lot of the priority 4 cases have been sacrificed to improve priorities 1-3. There are definite improvements in Priority 2 and 3 cases, while the Priority 1 cases remained the same. It was hoped that priority 1 would improve as well and maybe that will happen after double coverage is implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVERAGE FOR EACH SIMULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metrics = {k:v for k,v in dataset.items() if \"metrics\" in k}\n",
    "\n",
    "def draw_coverage_basic(metrics, name):\n",
    "\n",
    "    coverages = [(pandas.to_datetime(arr[0]), float(arr[1]), float(arr[2])) for arr in metrics.values]\n",
    "    coverages.sort()\n",
    "\n",
    "    xs = [cov[0] for cov in coverages]\n",
    "    ys = [cov[1] for cov in coverages]\n",
    "    ys2 = [cov[2] for cov in coverages]\n",
    "\n",
    "    first = xs[0]\n",
    "    xs = [(date-first).total_seconds()/3600 for date in xs]\n",
    "\n",
    "    # Smoothed the original\n",
    "    smooth_xs = np.linspace(int(min(xs)), int(max(xs)), len(xs)//4)\n",
    "\n",
    "    spl = make_interp_spline(xs, ys, k=1)\n",
    "    smooth_ys = spl(smooth_xs)\n",
    "    \n",
    "    spl2 = make_interp_spline(xs, ys2, k=1)\n",
    "    smooth_ys2 = spl2(smooth_xs)\n",
    "\n",
    "    name = name.split('/')[0].replace('-', ' ')\n",
    "    figure_title = \"Coverage: \" + name\n",
    "    plt.figure(figure_title)\n",
    "    plt.title(figure_title)\n",
    "    plt.xlim(0,24)\n",
    "    plt.ylim(-1, 100)\n",
    "#     plt.xlabel(\"Number of hours since starting\"); \n",
    "    plt.ylabel('Coverage')\n",
    "\n",
    "    plt.plot(smooth_xs, smooth_ys, '-', color='blue', label='Primary coverage')\n",
    "    plt.plot(smooth_xs, smooth_ys2, '-', color='teal', label='Secondary coverage')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "for name, metric in metrics.items(): \n",
    "#     draw_coverage_basic(metric, name)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments: \n",
    "1. Just based on the primary coverage (double is currently missing), it seems like the BTT finished all the cases the soonest. However, as we saw above with the travel times, it's interesting because less cases were reached within r1 using btt. Again, with the travel times, it's going to be interesting to see whether more high priority cases were reached (or maybe **all of the disaster cases** are high/top priority? Probably requires more thought...) And also the double coverage is going to paint a better picture of what's happening. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_coverage_overlayed(metrics, name, colors, level, ax = None, xlim_cap=24, ticks=8):\n",
    "\n",
    "    coverages = [(pandas.to_datetime(arr[0]), float(arr[1]), float(arr[2])) for arr in metrics.values]\n",
    "    coverages.sort()\n",
    "\n",
    "    xs = [cov[0] for cov in coverages]\n",
    "    ys = [cov[1] for cov in coverages]\n",
    "    ys2 = [cov[2] for cov in coverages]\n",
    "    \n",
    "    plt.bar([3,9, 15, 21], [100,100,100,100], width=5.5, color='whitesmoke')\n",
    "    print(\"Name is {}\".format(name))\n",
    "    if 'disaster' in name:\n",
    "        plt.bar([14], [100], width=1, color='pink')\n",
    "\n",
    "    first = xs[0]\n",
    "    xs = [(date-first).total_seconds()/3600 for date in xs]\n",
    "\n",
    "    # Smoothed the original\n",
    "    smooth_xs = np.linspace(int(min(xs)), int(max(xs)), len(xs)//4)\n",
    "\n",
    "    spl = make_interp_spline(xs, ys, k=1)\n",
    "    smooth_ys = spl(smooth_xs)\n",
    "    \n",
    "    spl2 = make_interp_spline(xs, ys2, k=1)\n",
    "    smooth_ys2 = spl2(smooth_xs)\n",
    "\n",
    "\n",
    "    figure_title = \"Coverage: overlayed\"\n",
    "    plt.ylim(-1, 80)\n",
    "    plt.xlim(0, xlim_cap)\n",
    "    plt.xticks(range(0, xlim_cap, xlim_cap//ticks))\n",
    "    plt.ylabel('Coverage')\n",
    "    \n",
    "    name = name.split('/')[0].replace('-', ' ')\n",
    "    plot_outputs = []\n",
    "    color = colors.pop() if colors else None\n",
    "    \n",
    "    if 'primary' in level: \n",
    "        plot_outputs.append(plt.plot(smooth_xs, smooth_ys, '-', color=color, label=name + \" primary\"))\n",
    "    color = colors.pop() if colors else None\n",
    "    \n",
    "    if 'secondary' in level: \n",
    "        plot_outputs.append(plt.plot(smooth_xs, smooth_ys2, '-', color=color, label=name + \" secondary\"))\n",
    "    \n",
    "    return plt, smooth_xs, plot_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_overlay(term):\n",
    "    plt.figure()\n",
    "    colors = ['blue', 'teal', 'red', 'orange', 'black', 'grey']\n",
    "    for name, metric in metrics.items(): \n",
    "        if term in name: \n",
    "            plot_colors = None\n",
    "            if colors:\n",
    "                plot_colors = [colors.pop(), colors.pop()]\n",
    "            print(name, plot_colors)\n",
    "            draw_coverage_overlayed(metric, name, plot_colors, ['primary', 'secondary'], ax=plt)\n",
    "    print(\"Note: the colors are [secondary color, primary color].\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_overlay(\"weekday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_overlay(\"weekend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overlay(\"disaster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overlay(\"best-travel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overlay(\"coverage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_overlay(\"weighted-dispatch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the travel times duration points with the coverage already implemented above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific plots:\n",
    "- Weekend: Comparing the primary coverage vs secondary coverage for each policy\n",
    "- Disaster: Comparing primary vs secondary \n",
    "- Weekend vs disaster: primary against primary \n",
    "- Weekend vs disaster: secondary against secondary \n",
    "- Disaster: primary coverage y-axis and travel times y-axis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_overlay_specific(term, level):\n",
    "    plt.figure()\n",
    "    colors = ['blue', 'teal', 'red', 'orange', 'black', 'grey']\n",
    "    for name, metric in metrics.items(): \n",
    "        if term in name: \n",
    "            plot_colors = None\n",
    "            if colors:\n",
    "                plot_colors = [colors.pop(), colors.pop()]\n",
    "            print(name, plot_colors)\n",
    "            draw_coverage_overlayed(metric, name, plot_colors, level)\n",
    "    print(\"Note: the colors are [secondary color, primary color].\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overlay_specific('weekend', ['primary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overlay_specific('disaster', ['primary'])\n",
    "plt.show()\n",
    "plot_overlay_specific('disaster', ['secondary'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overlay_specific('weighted-dispatch', ['primary'])\n",
    "plt.show()\n",
    "plot_overlay_specific('weighted-dispatch', ['secondary'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the travel times for all cases regardless of event severity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = {k:v for k,v in dataset.items() if \"processed_cases\" in k}\n",
    "severities = lambda term: {name: [p for p in caseset.priority] for name, caseset in cases.items() if term in name}\n",
    "raw_start_times = lambda term: {name: [pandas.to_datetime(time) for time in caseset.start_time] for name, caseset in cases.items() if term in name}\n",
    "get_start_times = lambda term: {name:[np.float((time-times[0]).total_seconds())/3600 for time in times] for name, times in raw_start_times(term).items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function s.t. given a search term, if the term is in the caseset then plot it.\n",
    "# Almost exactly the same logic as the cleaned_cases above. \n",
    "\n",
    "cases = {k:v for k,v in dataset.items() if \"processed_cases\" in k}\n",
    "\n",
    "# Grab the indices which store the durations\n",
    "indices = []\n",
    "for name, caseset in cases.items(): \n",
    "    keys = caseset.keys()\n",
    "    for index in range(len(keys)):\n",
    "        if 'duration' in keys[index]: \n",
    "#             print(index, keys[index])\n",
    "            indices += [index]\n",
    "        \n",
    "    break\n",
    "\n",
    "# For each case, determine the total duration. List of casesets, which contain list of cases. Use transformations. \n",
    "\n",
    "# [[[case[index] for index in indices] for case in caseset] for caseset in cases]\n",
    "\n",
    "cleaned_cases = {name: [{column_name: [pandas.to_timedelta(d).total_seconds()/60 for d in data]} for column_name, data in caseset.items() if \"duration\" in column_name] for name, caseset in cases.items()}\n",
    "l = cleaned_cases['tj-weekday-best-travel/processed_cases.csv'] # Match the scenario and dispatch policy\n",
    "match = any(['TO_INCIDENT' in k for d in l for k in d.keys()]) # Match the event 1, so this should be hardcoded\n",
    "\n",
    "def get_duration_times(term):    \n",
    "    # Plot once for each case set whose name contains the term.\n",
    "    casesets = {name: caseset for name, caseset in cleaned_cases.items() if term in name}\n",
    "    \n",
    "    # We only want the TO INCIDENT list \n",
    "    durations = {name: list(each_dict.values())[0] for name, caseset in casesets.items() for each_dict in caseset\\\n",
    "                if any([\"TO_INCIDENT\" in key for key in each_dict.keys()])}\n",
    "    \n",
    "    return durations\n",
    "\n",
    "r = get_duration_times('weekend')\n",
    "len(r) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_overlay_both(term, level, legend=True, highpriority=False, xlim_cap=24, ticks=8):\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    colors = ['blue', 'teal', 'red', 'orange', 'black', 'grey']\n",
    "    lns = []\n",
    "    \n",
    "    term = term.replace('tj-', '')\n",
    "    title = term.replace('-', ' ').title()\n",
    "\n",
    "    \n",
    "    plt.title(title)\n",
    "    \n",
    "    for name, metric in metrics.items(): \n",
    "        if term in name: \n",
    "            plot_colors = None\n",
    "            if colors:\n",
    "                plot_colors = [colors.pop(), colors.pop()]\n",
    "            print(name, plot_colors)\n",
    "            ax1, smooth_xs, ln = draw_coverage_overlayed(metric, name, plot_colors, level, \n",
    "                                                         ax=ax1, xlim_cap=xlim_cap, ticks=ticks)\n",
    "            print(ln)\n",
    "            lns+=ln\n",
    "            print(lns)\n",
    "    print(\"Note: the colors are [secondary color, primary color].\")\n",
    "#     plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "#     print(smooth_xs)\n",
    "#     print(type(smooth_xs[0]))\n",
    "    \n",
    "#     ax1.legend(loc=0)\n",
    "\n",
    "    running_mean = lambda x, N:  np.convolve(x, np.ones((N,))/N)[(N-1):]\n",
    "    \n",
    "    timeset = get_duration_times(term)\n",
    "#     ax2 = ax1.twinx()\n",
    "    ax2 = plt #Back to separating\n",
    "\n",
    "    if highpriority:  title += \" High Priority\"\n",
    "    plt.title(title)\n",
    "    plt.bar([3,9, 15, 21], [100,100,100,100], width=5.5, color='whitesmoke')\n",
    "    if 'disaster' in term:\n",
    "        plt.bar([14], [100], width=1, color='pink')\n",
    "        \n",
    "    # r1, r2\n",
    "    lnr1 = ax2.plot([0,24.], [10,10], 'r--', label='r1')\n",
    "    lnr2 = ax2.plot([0,24.], [14,14], 'y--', label='r2')\n",
    "    \n",
    "    \n",
    "    start_times = get_start_times(term)\n",
    "    for name, times in timeset.items():\n",
    "        label = name.split('/')[0].replace('-', ' ') + \" times\"\n",
    "        print(\"plotting \", label)\n",
    "        \n",
    "        if highpriority:\n",
    "            severity_list = severities(term)\n",
    "#             print(severity_list)\n",
    "            high_start_times = [start_times[name][i] for i in range(len(severity_list[name])) \\\n",
    "                                if severity_list[name][i] in [1,2, 3]]\n",
    "            high_times = [times[i] for i in range(len(severity_list[name])) \\\n",
    "                          if severity_list[name][i] in [1,2,3 ]]\n",
    "#             print(len(high_times))\n",
    "            lns += [ax2.plot(high_start_times, high_times, '.', label=label)]\n",
    "            lns += [ax2.plot(high_start_times, running_mean(high_times, 4), '-', \n",
    "                         color='purple', label=term + ' running average')]\n",
    "            \n",
    "        else:\n",
    "            lns += [ax2.plot(start_times[name], times, '.', label=label)]\n",
    "            lns += [ax2.plot(start_times[name], running_mean(times, 4), '-', \n",
    "                         color='black', label=term + ' running average')]\n",
    "        \n",
    "        # Calculate the moving average here\n",
    "        \n",
    "        \n",
    "        \n",
    "        plt.ylabel(\"Minutes for TO_INCIDENT\")\n",
    "        plt.xticks(range(0, xlim_cap, xlim_cap//ticks))\n",
    "        plt.xlim([0, xlim_cap])\n",
    "        plt.ylim([0, 40])\n",
    "        \n",
    "#         ax2.set_ylabel(\"Minutes for TO_INCIDENT\")\n",
    "#         ax2.set_xlim([0,24])\n",
    "#         ax2.set_ylim([0, 50])\n",
    "        \n",
    "    \n",
    "    print(\"Show.\")\n",
    "\n",
    "\n",
    "\n",
    "    # Legends are special. \n",
    "#     lns = [ln[0] for ln in lns]\n",
    "#     lns += lnr1 + lnr2\n",
    "#     labs = [l.get_label() for l in lns]\n",
    "    \n",
    "#     if legend: plt.legend(lns, labs, loc=0)\n",
    "\n",
    "\n",
    "#     plt.legend()\n",
    "    if highpriority:  term += \"-high-priority\" \n",
    "\n",
    "    plt.show()\n",
    "\n",
    "coverage_levels = ['primary', 'secondary']\n",
    "\n",
    "simulation = [\"-\".join([scenario, policy]) \\\n",
    "              for scenario in ['disaster', 'weekend', 'weekday'] \\\n",
    "              for policy in ['best-travel','best-coverage','weighted-dispatch']]\n",
    "    \n",
    "\n",
    "simulation = ['tj-' + s for s in simulation]\n",
    "simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for s in simulation:\n",
    "    plot_overlay_both(s, coverage_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for s in simulation:\n",
    "    plot_overlay_both(s, coverage_levels, highpriority=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Scaling the Disaster Lambda Rates\n",
    "\n",
    "#### We want to see the effect of scaling the lambda on the coverage. \n",
    "\n",
    "#### DEBUG THE DISASTER SCENARIO HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overlay_both('depleting', coverage_levels, xlim_cap=96, ticks=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NORMAL WEEKDAYS AND WEEKENDS SCENARIOS, XLIM 1 WEEK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overlay_both('scaling-weeks-best-travel', coverage_levels, xlim_cap=168, ticks=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overlay_both('scaling-weeks-best-coverage', coverage_levels, xlim_cap=168, ticks=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
